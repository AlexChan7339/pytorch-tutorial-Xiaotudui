# 21-线形层及其他层价绍

## 21.1-正则化层

采用正则化层（用的比较少）可以提高训练速度

![image-20251027221318916](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102810.png)

![image-20251027221409560](https://typora3.oss-cn-shanghai.aliyuncs.com/202510272214708.png)

![image-20251027221421592](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102694.png)

- num_features:前面提到的channel通道数

![image-20251027221542353](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102719.png)

将输入放入正则化层的网络当中

## 21.2-Recurrent层

主要用于文字识别的网络结构，用的概率不是很多

![image-20251027221756496](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102760.png)

## 21.3-Transformer层

![image-20251027221823131](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102771.png)

## 21.4-线性层

![image-20251027221903887](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102827.png)

![image-20251027221923153](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102922.png)

![image-20251027222734071](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102031.png)

上图中每个神经元之间采用线性连接方式，InputLayer --- > Hidden Layer中in_features=d   (包括$x_1,x_2,...,x_d$)，对应的out_features=L(包括$g_1,g_2,...,g_L$)，$g_1$是由$x_1,x_2,...,x_d$计算得到，计算公式是$k_1 * x_1+b_1+k_2*x_2+b_2+...+k_d*x_d+b_d$,神经网络训练主要是对$k_1, k_2, ..., k_d$进行调整

![image-20251027223251961](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102188.png)

k（权重）和b（偏置）的初始化主要是从均匀分布中进行采样

![image-20251027223448138](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102229.png)

---

假设有一张5*5图片，将其展成一行，也就是25个，将这25个通过**线性层**变为3个元素

![image-20251027223833766](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102340.png)

在pycharm项目下新建python脚本nn_linear

```python
import torchvision

dataset = torchvision.datasets.CIFAR10("./dataset.CIFAR10", train=False, transform=torchvision.transforms.ToTensor(), download=True)
dataloader = DataLoader(dataset, batch_size=64, drop_last=true)

class Tudui(nn.Module):
    def __init__(self):
        super(Tudui, self).__init__()
        self.Linear1 = Linear(3072, 10)
        
    def forward(self, input):
        output = self.linear1(input)
        return output
tudui = Tudui()

for data in dataloader:
    img, targets = data
    print(imgs.shape)
    # torch.Size([64, 3, 32, 32])
    # 3*32*32=3-72
    # 我们将其拉伸为一维向量
    output = torch.reshape(imgs, (64, 1, 1, -1))
    print(output.shape)
    # torch.Size([64, 1, 1, 3072])
    output = tudui(output)
    print(output.shape)

```



## 21.5-Dropout  层

在训练过程中，会随机将输入tensor的元素变为0，随机的概率为p，防止过拟合

![image-20251027222131151](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102493.png)

![image-20251027222143514](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102679.png)

## 21.6-Sparse层

主要用于自然语言处理中

![image-20251027222235084](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102692.png)

## 21.7-Distance Functions

计算两个值之间的误差

![image-20251027222329845](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102753.png)

## 21.8-Loss Functions

计算损失函数

![image-20251027222421448](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062102817.png)

......