# 20-非线性激活

非线性激活主要是给神经网络引入非线性特性(非线性特征越多，才可以训练出符合各种特征的模型，提高模型泛化能力)，没有激活函数的神经网络实际上是线性可加的（比如前面的池化和卷积，都可以看成是线性回归模型），那么多线性层其实可以归为一层。只具有线性的神经网络表达能力极其有限。常见的非线性激活有

- ReLU(x)=max(0, x)

  ![image-20251026223741097](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062101258.png)

- N:batch_size(下下图中)

- *：可以是任意形状

![image-20251026223828135](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062101171.png)

- Sigmoid()函数:会把输入的数值压缩到0和1之间,适合用在二分类任务的输出层。

  ![image-20251026223943437](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062101218.png)

pycharm项目下新建python文件nn_relu

```python
import torch
from torch import nn
from torch.nn import ReLU

input = torch.tensor([[1, -0.5],
                     [-1, 3]])

# 需要指定batch_size
input = torch.reshape(input, (-1, 1, 2, 2))
print(input.shape)
# torch.Size([1, 1, 2, 2])

class Tudui(nn.Module):
    def __init__(self):
        super(Tudui, self).__init__()
        self.relu1 = ReLU()
        
    def forward(self, input):
        output = self.relu1(input)
        return output
    
tudui = Tudui()
output = tudui(input)
print(output)
```

解释ReLU的参数inplace: 是否就地修改（常用和默认值都是False，防止原始数据被修改和丢失）

![image-20251026224652084](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062101204.png)

上述代码结果：负数都被截断了，用0代替，整数仍然被保留

![image-20251026225015163](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062101151.png)

> ReLU函数对图像的作用不明显，Sigmoid较为明显

```python
import torch
import torchvision
from torch import nn
from torch.nn import ReLU, Sigmoid
from torch.utils.data import Dataset, DataLoader


input = torch.tensor([[1, -0.5],
                     [-1, 3]])

# 需要指定batch_size
input = torch.reshape(input, (-1, 1, 2, 2))
print(input.shape)
# torch.Size([1, 1, 2, 2])
dataset = torchvision.datasets.CIFAR10("./dataset/CIFAR10", trian=False, download=True, transform=torchvision.transforms.ToTensor())
dataloader = DataLoader(dataset, batch_size=64)

class Tudui(nn.Module):
    def __init__(self):
        super(Tudui, self).__init__()
        self.relu1 = ReLU()
        self.sigmoid1 = Sigmoid()
        
    def forward(self, input):
        output = self.sigmoid1(input)
        return output
    
tudui = Tudui()

step = 0
writer = SummaryWirter("./logs_sigmoid")
for data in dataloder:
    imgs, targets = data
    writer.add_images("input", imgs, step)
    output = tudui(imgs)
    writer.add_images("output", output, step)
    step = step + 1
    
writer.close()
```

在terminal 中输入tensorboard --logdir=logs_sigmoid,点击输出的端口，在弹出的网页中可以看到输出结果

![image-20251026230205939](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062101271.png)

![image-20251026230216048](https://typora3.oss-cn-shanghai.aliyuncs.com/202511062101633.png)